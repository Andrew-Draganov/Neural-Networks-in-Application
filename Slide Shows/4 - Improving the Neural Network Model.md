Lecture 4 dove deeper into the neural network architecture, specifically focusing on methods to reduce overfitting and speed up learning.
Information covered included:

1) L2 Regularization
2) Dropout
3) Batch Normalization
4) Xavier Initialization (derivation of the 1/n_in variance that can be found [here](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization))
5) Momentum Gradient Descent Optimizer
6) Adagrad Optimizer

This information was presented as mathematical formulas with explanations of why they make sense in the context of a neural network.  My lecture notes for this material can be found [here](https://github.com/Andrew-Draganov/Neural-Networks-in-Application/tree/master/Slide%20Shows/4-LectureNotes).
